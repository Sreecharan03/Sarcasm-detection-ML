{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from textblob) (3.9.2)\n",
      "Requirement already satisfied: click in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nltk>=3.9->textblob) (8.3.1)\n",
      "Requirement already satisfied: joblib in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nltk>=3.9->textblob) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nltk>=3.9->textblob) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nltk>=3.9->textblob) (4.67.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
      "Requirement already satisfied: requests in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from vaderSentiment) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->vaderSentiment) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->vaderSentiment) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->vaderSentiment) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->vaderSentiment) (2025.11.12)\n",
      "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Setting up NLTK resources...\n",
      "âœ… punkt already downloaded\n",
      "ðŸ“¥ Downloading stopwords...\n",
      "ðŸ“¥ Downloading vader_lexicon...\n",
      "ðŸ“¥ Downloading wordnet...\n",
      "ðŸ“¥ Downloading averaged_perceptron_tagger...\n",
      "\n",
      "ðŸ§¹ Text Preprocessing & Feature Engineering Pipeline\n",
      "=======================================================\n",
      "ðŸ“‚ Loading datasets from data exploration...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Headlines: 28,619 samples\n",
      "âœ… Reddit: 50,000 samples (sampled)\n",
      "\n",
      "ðŸŽ¯ Preprocessing Functions Defined:\n",
      "  âœ… Text cleaning (preserves sarcasm patterns)\n",
      "  âœ… NLTK tools initialized\n",
      "  âœ… Sentiment analyzer ready\n",
      "  âœ… Datasets loaded and ready\n",
      "\n",
      "ðŸ“‹ Next Steps:\n",
      "1. Apply text cleaning to both datasets\n",
      "2. Extract contextual features (sentiment, incongruity)\n",
      "3. Create hybrid feature vectors\n",
      "4. Split data for training/validation/testing\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Text Preprocessing & Feature Engineering Setup\n",
    "# This cell sets up the preprocessing pipeline for hybrid CNN + contextual features\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Download NLTK data (run once)\n",
    "print(\"ðŸ”§ Setting up NLTK resources...\")\n",
    "nltk_downloads = ['punkt', 'stopwords', 'vader_lexicon', 'wordnet', 'averaged_perceptron_tagger']\n",
    "for resource in nltk_downloads:\n",
    "    try:\n",
    "        nltk.data.find(f'tokenizers/{resource}')\n",
    "        print(f\"âœ… {resource} already downloaded\")\n",
    "    except LookupError:\n",
    "        print(f\"ðŸ“¥ Downloading {resource}...\")\n",
    "        nltk.download(resource, quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "print(\"\\nðŸ§¹ Text Preprocessing & Feature Engineering Pipeline\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Initialize preprocessing tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load our datasets from previous exploration\n",
    "print(\"ðŸ“‚ Loading datasets from data exploration...\")\n",
    "try:\n",
    "    # Load headlines dataset\n",
    "    with open('data/raw/Sarcasm_Headlines_Dataset_v2.json', 'r') as f:\n",
    "        headlines_data = [json.loads(line) for line in f]\n",
    "    headlines_df = pd.DataFrame(headlines_data)\n",
    "    \n",
    "    # Load reddit dataset (sample for processing speed)\n",
    "    reddit_df = pd.read_csv('data/raw/train-balanced-sarcasm.csv')\n",
    "    reddit_sample = reddit_df.sample(n=50000, random_state=42)  # 50K sample for training\n",
    "    \n",
    "    print(f\"âœ… Headlines: {len(headlines_df):,} samples\")\n",
    "    print(f\"âœ… Reddit: {len(reddit_sample):,} samples (sampled)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading data: {e}\")\n",
    "\n",
    "# Define text cleaning function\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Advanced text cleaning for sarcasm detection\n",
    "    Preserves some punctuation patterns important for sarcasm\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove URLs but keep the structure for sarcasm detection\n",
    "    text = re.sub(r'http\\S+|www\\S+', '[URL]', text)\n",
    "    \n",
    "    # Handle contractions (keep some for sarcasm patterns)\n",
    "    contractions = {\n",
    "        \"won't\": \"will not\", \"can't\": \"cannot\", \"n't\": \" not\",\n",
    "        \"'re\": \" are\", \"'ve\": \" have\", \"'ll\": \" will\", \"'d\": \" would\"\n",
    "    }\n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    \n",
    "    # Remove extra whitespace but preserve sentence structure\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove excessive punctuation but keep some for sarcasm detection\n",
    "    text = re.sub(r'[!]{2,}', '!!', text)  # Max 2 exclamation marks\n",
    "    text = re.sub(r'[?]{2,}', '??', text)  # Max 2 question marks\n",
    "    text = re.sub(r'[.]{3,}', '...', text)  # Standardize ellipsis\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"\\nðŸŽ¯ Preprocessing Functions Defined:\")\n",
    "print(\"  âœ… Text cleaning (preserves sarcasm patterns)\")\n",
    "print(\"  âœ… NLTK tools initialized\") \n",
    "print(\"  âœ… Sentiment analyzer ready\")\n",
    "print(\"  âœ… Datasets loaded and ready\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Next Steps:\")\n",
    "print(\"1. Apply text cleaning to both datasets\")\n",
    "print(\"2. Extract contextual features (sentiment, incongruity)\")\n",
    "print(\"3. Create hybrid feature vectors\")\n",
    "print(\"4. Split data for training/validation/testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Applying Text Cleaning & Feature Extraction...\n",
      "==================================================\n",
      "1ï¸âƒ£ Processing Headlines Dataset...\n",
      "   ðŸ“Š Extracting contextual features...\n",
      "      Processing headline 0/28,619\n",
      "      Processing headline 5,000/28,619\n",
      "      Processing headline 10,000/28,619\n",
      "      Processing headline 15,000/28,619\n",
      "      Processing headline 20,000/28,619\n",
      "      Processing headline 25,000/28,619\n",
      "   âœ… Headlines processed: 28619 samples\n",
      "   âœ… Features extracted: 18 contextual features\n",
      "\n",
      "2ï¸âƒ£ Processing Reddit Dataset...\n",
      "   ðŸ“Š Extracting contextual features...\n",
      "      Processing comment 0/50,000\n",
      "      Processing comment 10,000/50,000\n",
      "      Processing comment 20,000/50,000\n",
      "      Processing comment 30,000/50,000\n",
      "      Processing comment 40,000/50,000\n",
      "   âœ… Reddit processed: 50000 samples\n",
      "   âœ… Features extracted: 18 contextual features\n",
      "\n",
      "ðŸ“Š CONTEXTUAL FEATURES EXTRACTED:\n",
      "===================================\n",
      "âœ… Sentiment: sentiment_compound, sentiment_polarity, sentiment_subjectivity\n",
      "âœ… Incongruity: positive_words, negative_words, sentiment_incongruity\n",
      "âœ… Hyperbole: exclamation_count, caps_ratio, intensifier_count\n",
      "âœ… Lexical: word_count, avg_word_length, quote_count\n",
      "\n",
      "ðŸŽ¯ Next: Combine datasets and create train/validation/test splits\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Text Cleaning & Contextual Feature Extraction\n",
    "# Extract linguistic features: sentiment polarity variation, incongruity, hyperbole patterns\n",
    "\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "print(\"ðŸ§¹ Applying Text Cleaning & Feature Extraction...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def extract_contextual_features(text):\n",
    "    \"\"\"\n",
    "    Extract contextual features for sarcasm detection as per research abstract:\n",
    "    - Sentiment polarity variation\n",
    "    - Incongruity patterns  \n",
    "    - Hyperbole detection\n",
    "    - Lexical pattern signals\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # 1. SENTIMENT POLARITY VARIATION\n",
    "    sentiment_scores = sentiment_analyzer.polarity_scores(text)\n",
    "    features['sentiment_compound'] = sentiment_scores['compound']\n",
    "    features['sentiment_positive'] = sentiment_scores['pos']\n",
    "    features['sentiment_negative'] = sentiment_scores['neg']\n",
    "    features['sentiment_neutral'] = sentiment_scores['neu']\n",
    "    \n",
    "    # Sentiment contradiction (positive words + negative sentiment = potential sarcasm)\n",
    "    blob = TextBlob(text)\n",
    "    features['sentiment_polarity'] = blob.sentiment.polarity\n",
    "    features['sentiment_subjectivity'] = blob.sentiment.subjectivity\n",
    "    \n",
    "    # 2. INCONGRUITY PATTERNS\n",
    "    # Detect contradictory sentiment words in same text\n",
    "    positive_words = ['great', 'wonderful', 'amazing', 'perfect', 'brilliant', 'fantastic', 'excellent']\n",
    "    negative_words = ['terrible', 'awful', 'horrible', 'worst', 'hate', 'stupid', 'ridiculous']\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    pos_count = sum(1 for word in positive_words if word in text_lower)\n",
    "    neg_count = sum(1 for word in negative_words if word in text_lower)\n",
    "    \n",
    "    features['positive_words'] = pos_count\n",
    "    features['negative_words'] = neg_count\n",
    "    features['sentiment_incongruity'] = 1 if (pos_count > 0 and neg_count > 0) else 0\n",
    "    \n",
    "    # 3. HYPERBOLE DETECTION\n",
    "    # Excessive punctuation patterns\n",
    "    features['exclamation_count'] = text.count('!')\n",
    "    features['question_count'] = text.count('?')\n",
    "    features['caps_ratio'] = sum(1 for c in text if c.isupper()) / (len(text) + 1)\n",
    "    \n",
    "    # Intensifier words\n",
    "    intensifiers = ['so', 'very', 'extremely', 'absolutely', 'totally', 'completely', 'really']\n",
    "    features['intensifier_count'] = sum(1 for word in intensifiers if word in text_lower)\n",
    "    \n",
    "    # 4. LEXICAL PATTERN SIGNALS\n",
    "    # Text length and complexity\n",
    "    words = text.split()\n",
    "    features['word_count'] = len(words)\n",
    "    features['char_count'] = len(text)\n",
    "    features['avg_word_length'] = np.mean([len(word) for word in words]) if words else 0\n",
    "    \n",
    "    # Quotation marks (often used in sarcastic contexts)\n",
    "    features['quote_count'] = text.count('\"') + text.count(\"'\")\n",
    "    \n",
    "    # Ellipsis usage\n",
    "    features['ellipsis_count'] = text.count('...')\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Apply cleaning and feature extraction to Headlines dataset\n",
    "print(\"1ï¸âƒ£ Processing Headlines Dataset...\")\n",
    "headlines_df['cleaned_text'] = headlines_df['headline'].apply(clean_text)\n",
    "\n",
    "# Extract contextual features for headlines\n",
    "print(\"   ðŸ“Š Extracting contextual features...\")\n",
    "headline_features = []\n",
    "for idx, text in enumerate(headlines_df['cleaned_text']):\n",
    "    if idx % 5000 == 0:\n",
    "        print(f\"      Processing headline {idx:,}/{len(headlines_df):,}\")\n",
    "    features = extract_contextual_features(text)\n",
    "    headline_features.append(features)\n",
    "\n",
    "# Convert to DataFrame\n",
    "headline_features_df = pd.DataFrame(headline_features)\n",
    "headline_final = pd.concat([headlines_df.reset_index(drop=True), headline_features_df], axis=1)\n",
    "\n",
    "print(f\"   âœ… Headlines processed: {len(headline_final)} samples\")\n",
    "print(f\"   âœ… Features extracted: {len(headline_features_df.columns)} contextual features\")\n",
    "\n",
    "# Apply cleaning and feature extraction to Reddit dataset\n",
    "print(\"\\n2ï¸âƒ£ Processing Reddit Dataset...\")\n",
    "reddit_sample['cleaned_text'] = reddit_sample['comment'].apply(clean_text)\n",
    "\n",
    "print(\"   ðŸ“Š Extracting contextual features...\")\n",
    "reddit_features = []\n",
    "for idx, text in enumerate(reddit_sample['cleaned_text']):\n",
    "    if idx % 10000 == 0:\n",
    "        print(f\"      Processing comment {idx:,}/{len(reddit_sample):,}\")\n",
    "    features = extract_contextual_features(text)\n",
    "    reddit_features.append(features)\n",
    "\n",
    "# Convert to DataFrame\n",
    "reddit_features_df = pd.DataFrame(reddit_features)\n",
    "reddit_final = pd.concat([reddit_sample.reset_index(drop=True), reddit_features_df], axis=1)\n",
    "\n",
    "print(f\"   âœ… Reddit processed: {len(reddit_final)} samples\")\n",
    "print(f\"   âœ… Features extracted: {len(reddit_features_df.columns)} contextual features\")\n",
    "\n",
    "# Display feature summary\n",
    "print(f\"\\nðŸ“Š CONTEXTUAL FEATURES EXTRACTED:\")\n",
    "print(\"=\" * 35)\n",
    "feature_categories = {\n",
    "    'Sentiment': ['sentiment_compound', 'sentiment_polarity', 'sentiment_subjectivity'],\n",
    "    'Incongruity': ['positive_words', 'negative_words', 'sentiment_incongruity'],\n",
    "    'Hyperbole': ['exclamation_count', 'caps_ratio', 'intensifier_count'],\n",
    "    'Lexical': ['word_count', 'avg_word_length', 'quote_count']\n",
    "}\n",
    "\n",
    "for category, features in feature_categories.items():\n",
    "    print(f\"âœ… {category}: {', '.join(features)}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Next: Combine datasets and create train/validation/test splits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Combining Datasets & Creating Splits...\n",
      "=============================================\n",
      "1ï¸âƒ£ Preparing datasets for combination...\n",
      "   âœ… Headlines: 28619 samples\n",
      "   âœ… Reddit: 50000 samples\n",
      "\n",
      "2ï¸âƒ£ Combining datasets...\n",
      "   âœ… Combined dataset: 78,619 samples\n",
      "   âœ… Total sarcastic: 38,696 (49.2%)\n",
      "   âœ… Total non-sarcastic: 39,923\n",
      "   âœ… Feature columns: 18\n",
      "\n",
      "3ï¸âƒ£ Creating train/validation/test splits...\n",
      "   ðŸ“Š Train set: 62,895 samples (49.2% sarcastic)\n",
      "   ðŸ“Š Validation set: 7,862 samples (49.2% sarcastic)\n",
      "   ðŸ“Š Test set: 7,862 samples (49.2% sarcastic)\n",
      "\n",
      "4ï¸âƒ£ Saving preprocessed data...\n",
      "   âœ… Saved train_data.csv: 62,895 samples\n",
      "   âœ… Saved val_data.csv: 7,862 samples\n",
      "   âœ… Saved test_data.csv: 7,862 samples\n",
      "   âœ… Saved feature_info.json\n",
      "\n",
      "ðŸ“‹ SAMPLE OF PROCESSED DATA:\n",
      "================================\n",
      "[NORMAL] amazing couple adopts 7 siblings to keep them together\n",
      "         Sentiment: 0.670, Words: 1+/0-\n",
      "[SARCASTIC] 92% of area woman's holiday recipes involve pulverizing bag ...\n",
      "         Sentiment: 0.402, Words: 0+/0-\n",
      "[SARCASTIC] that's likely the case, so i will blame f.o.o.l for not call...\n",
      "         Sentiment: -0.395, Words: 0+/0-\n",
      "\n",
      "ðŸŽ¯ PREPROCESSING COMPLETE!\n",
      "==============================\n",
      "âœ… Text cleaning applied\n",
      "âœ… Contextual features extracted\n",
      "âœ… Train/val/test splits created\n",
      "âœ… Data saved for model training\n",
      "\n",
      "ðŸ“‚ Files saved in: data/processed/\n",
      "   - train_data.csv\n",
      "   - val_data.csv\n",
      "   - test_data.csv\n",
      "   - feature_info.json\n",
      "\n",
      "ðŸš€ Ready for Phase 2: Model Development!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Data Combination & Train/Validation/Test Splits\n",
    "# Combine datasets and create stratified splits for model training\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "print(\"ðŸ”„ Combining Datasets & Creating Splits...\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Prepare datasets for combination\n",
    "print(\"1ï¸âƒ£ Preparing datasets for combination...\")\n",
    "\n",
    "# Headlines dataset preparation\n",
    "headlines_processed = headline_final[['cleaned_text', 'is_sarcastic'] + list(headline_features_df.columns)].copy()\n",
    "headlines_processed['dataset_source'] = 'headlines'\n",
    "headlines_processed.rename(columns={'is_sarcastic': 'label'}, inplace=True)\n",
    "\n",
    "# Reddit dataset preparation  \n",
    "reddit_processed = reddit_final[['cleaned_text', 'label'] + list(reddit_features_df.columns)].copy()\n",
    "reddit_processed['dataset_source'] = 'reddit'\n",
    "\n",
    "print(f\"   âœ… Headlines: {len(headlines_processed)} samples\")\n",
    "print(f\"   âœ… Reddit: {len(reddit_processed)} samples\")\n",
    "\n",
    "# Combine datasets\n",
    "print(\"\\n2ï¸âƒ£ Combining datasets...\")\n",
    "combined_data = pd.concat([headlines_processed, reddit_processed], ignore_index=True)\n",
    "combined_data = combined_data.dropna(subset=['cleaned_text'])  # Remove any null texts\n",
    "\n",
    "print(f\"   âœ… Combined dataset: {len(combined_data):,} samples\")\n",
    "print(f\"   âœ… Total sarcastic: {combined_data['label'].sum():,} ({combined_data['label'].mean():.1%})\")\n",
    "print(f\"   âœ… Total non-sarcastic: {(combined_data['label'] == 0).sum():,}\")\n",
    "\n",
    "# Display feature columns\n",
    "feature_columns = [col for col in combined_data.columns if col not in ['cleaned_text', 'label', 'dataset_source']]\n",
    "print(f\"   âœ… Feature columns: {len(feature_columns)}\")\n",
    "\n",
    "# Create stratified splits\n",
    "print(\"\\n3ï¸âƒ£ Creating train/validation/test splits...\")\n",
    "\n",
    "# First split: 80% train, 20% temp (which will become 10% val, 10% test)\n",
    "X = combined_data[['cleaned_text'] + feature_columns]\n",
    "y = combined_data['label']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: Split the 20% into 10% validation, 10% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"   ðŸ“Š Train set: {len(X_train):,} samples ({y_train.mean():.1%} sarcastic)\")\n",
    "print(f\"   ðŸ“Š Validation set: {len(X_val):,} samples ({y_val.mean():.1%} sarcastic)\")\n",
    "print(f\"   ðŸ“Š Test set: {len(X_test):,} samples ({y_test.mean():.1%} sarcastic)\")\n",
    "\n",
    "# Save preprocessed data\n",
    "print(\"\\n4ï¸âƒ£ Saving preprocessed data...\")\n",
    "\n",
    "# Create processed data directory\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# Save splits as separate files\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "train_data.to_csv('data/processed/train_data.csv', index=False)\n",
    "val_data.to_csv('data/processed/val_data.csv', index=False)\n",
    "test_data.to_csv('data/processed/test_data.csv', index=False)\n",
    "\n",
    "print(f\"   âœ… Saved train_data.csv: {len(train_data):,} samples\")\n",
    "print(f\"   âœ… Saved val_data.csv: {len(val_data):,} samples\")\n",
    "print(f\"   âœ… Saved test_data.csv: {len(test_data):,} samples\")\n",
    "\n",
    "# Save feature information for model training\n",
    "feature_info = {\n",
    "    'feature_columns': feature_columns,\n",
    "    'text_column': 'cleaned_text',\n",
    "    'label_column': 'label',\n",
    "    'total_samples': len(combined_data),\n",
    "    'sarcasm_rate': float(combined_data['label'].mean()),\n",
    "    'train_size': len(train_data),\n",
    "    'val_size': len(val_data),\n",
    "    'test_size': len(test_data)\n",
    "}\n",
    "\n",
    "with open('data/processed/feature_info.json', 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "\n",
    "print(f\"   âœ… Saved feature_info.json\")\n",
    "\n",
    "# Display sample of processed data\n",
    "print(f\"\\nðŸ“‹ SAMPLE OF PROCESSED DATA:\")\n",
    "print(\"=\" * 32)\n",
    "sample_data = train_data.head(3)\n",
    "for idx, row in sample_data.iterrows():\n",
    "    label = \"SARCASTIC\" if row['label'] else \"NORMAL\"\n",
    "    text = row['cleaned_text'][:60] + \"...\" if len(row['cleaned_text']) > 60 else row['cleaned_text']\n",
    "    sentiment = row['sentiment_compound']\n",
    "    print(f\"[{label}] {text}\")\n",
    "    print(f\"         Sentiment: {sentiment:.3f}, Words: {row['positive_words']}+/{row['negative_words']}-\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ PREPROCESSING COMPLETE!\")\n",
    "print(\"=\" * 30)\n",
    "print(\"âœ… Text cleaning applied\")\n",
    "print(\"âœ… Contextual features extracted\") \n",
    "print(\"âœ… Train/val/test splits created\")\n",
    "print(\"âœ… Data saved for model training\")\n",
    "print(f\"\\nðŸ“‚ Files saved in: data/processed/\")\n",
    "print(\"   - train_data.csv\")\n",
    "print(\"   - val_data.csv\") \n",
    "print(\"   - test_data.csv\")\n",
    "print(\"   - feature_info.json\")\n",
    "\n",
    "print(f\"\\nðŸš€ Ready for Phase 2: Model Development!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
